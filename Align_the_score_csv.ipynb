{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "756f1a92",
   "metadata": {},
   "source": [
    "# Aggregated Evaluation Scoring\n",
    "\n",
    "This script merges and organizes scoring data from multiple evaluation systems applied to code documentation models. It builds a single comprehensive CSV containing all model outputs and associated scores from different scoring systems.\n",
    "\n",
    "## ðŸ”§ How It Works\n",
    "\n",
    "1. **Folder Setup**\n",
    "   - Each scoring system (Anthropic, DeepSeek, Gemini, GPT4o) has its own folder:\n",
    "     ```\n",
    "     Anthropic-Scoring/\n",
    "     DeepSeek-Scoring/\n",
    "     Gemini-Scoring/\n",
    "     GPT4o-Scoring/\n",
    "     ```\n",
    "   - Each folder contains CSV files named like:  \n",
    "     `evaluation_<model>.csv`  \n",
    "     Example: `evaluation_codellama_70b.csv`\n",
    "\n",
    "2. **Model Extraction**\n",
    "   - The script identifies documentation model names from the filenames.\n",
    "   - Scores are extracted from each file using `{model}_score` and `{model}_reason` columns.\n",
    "\n",
    "3. **Base File Initialization**\n",
    "   - The first file in the `Anthropic-Scoring` folder is used as a base template.\n",
    "   - It provides the common fields: `index`, `language`, `code`, and the model outputs.\n",
    "\n",
    "4. **Scoring Merge**\n",
    "   - For each scoring system and each model:\n",
    "     - The corresponding score and reasoning are added as new columns.\n",
    "     - All data is merged based on the `index` column.\n",
    "\n",
    "5. **Final Column Organization**\n",
    "   - Columns are reordered so that each model has its documentation followed by all scoring system results.\n",
    "\n",
    "6. **Output**\n",
    "   - âœ… `aggregated_evaluations.csv`: Final CSV with all documentation and scoring data merged and aligned.\n",
    "\n",
    "## ðŸ“¦ Output File Structure\n",
    "\n",
    "Each model's section includes:\n",
    "- Documented Output\n",
    "- Anthropic Score + Reason\n",
    "- DeepSeek Score + Reason\n",
    "- Gemini Score + Reason\n",
    "- GPT4o Score + Reason\n",
    "\n",
    "Example:\n",
    "```\n",
    "| index | language | code | codellama:70b | codellama:70b_Anthropic_score | codellama:70b_Anthropic_reason | ... |\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d54adef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated CSV has been saved to aggregated_evaluations.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the scoring systems and the corresponding folder names\n",
    "scoring_systems = {\n",
    "    \"Anthropic\": \"Anthropic-Scoring\",\n",
    "    \"DeepSeek\": \"DeepSeek-Scoring\",\n",
    "    \"Gemini\": \"Gemini-Scoring\",\n",
    "    \"GPT4o\": \"GPT4o-Scoring\"\n",
    "}\n",
    "\n",
    "# List of expected documentation model names in the base (order is important for final CSV)\n",
    "doc_models_order = [\"qwen2.5-coder:32b\", \"codellama:70b\", \"deepseek-coder:33b\", \"codegemma:7b\", \"codestral\"]\n",
    "\n",
    "# Function to derive the documentation model name from file name.\n",
    "def get_doc_model(filename):\n",
    "    # remove the prefix and .csv\n",
    "    model_str = filename.replace(\"evaluation_\", \"\").replace(\".csv\", \"\")\n",
    "    # if there's an underscore, assume the last underscore separates model and parameter info\n",
    "    if \"_\" in model_str:\n",
    "        parts = model_str.rsplit(\"_\", 1)\n",
    "        return parts[0] + \":\" + parts[1]\n",
    "    else:\n",
    "        return model_str\n",
    "\n",
    "# Create an empty dictionary to store scoring DataFrames by key (scoring system, doc_model)\n",
    "scoring_dfs = {}\n",
    "\n",
    "# Process each scoring system folder\n",
    "for system, folder in scoring_systems.items():\n",
    "    # List files in the folder\n",
    "    files = [f for f in os.listdir(folder) if f.endswith(\".csv\")]\n",
    "    for f in files:\n",
    "        doc_model = get_doc_model(f)\n",
    "        file_path = os.path.join(folder, f)\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Expect scoring columns: {doc_model}_score and {doc_model}_reason\n",
    "        score_col = f\"{doc_model}_score\"\n",
    "        reason_col = f\"{doc_model}_reason\"\n",
    "        if score_col not in df.columns or reason_col not in df.columns:\n",
    "            print(f\"Warning: {score_col} or {reason_col} not found in {file_path}\")\n",
    "            continue\n",
    "        # Extract only index and scoring columns\n",
    "        df_scoring = df[[\"index\", score_col, reason_col]].copy()\n",
    "        # Rename the columns to include the scoring system prefix\n",
    "        new_score_col = f\"{doc_model}_{system}_score\"\n",
    "        new_reason_col = f\"{doc_model}_{system}_reason\"\n",
    "        df_scoring.rename(columns={score_col: new_score_col, reason_col: new_reason_col}, inplace=True)\n",
    "        # Store in dictionary using key (system, doc_model)\n",
    "        scoring_dfs[(system, doc_model)] = df_scoring\n",
    "\n",
    "# Prepare a base DataFrame for common columns\n",
    "# Assume that the common columns (index, language, code, and all documented outputs) are identical across files.\n",
    "# We can use one file from one scoring system. For example, use the first file from the Anthropic folder.\n",
    "anthropic_folder = scoring_systems[\"Anthropic\"]\n",
    "anthropic_files = [f for f in os.listdir(anthropic_folder) if f.endswith(\".csv\")]\n",
    "if not anthropic_files:\n",
    "    raise Exception(\"No files found in Anthropic scoring folder.\")\n",
    "\n",
    "# Use the first file to get the base columns\n",
    "base_file_path = os.path.join(anthropic_folder, anthropic_files[0])\n",
    "base_df = pd.read_csv(base_file_path)\n",
    "\n",
    "# We assume base_df contains these columns: index, language, code, and columns for all documented outputs.\n",
    "base_columns = [\"index\", \"language\", \"code\"] + doc_models_order\n",
    "# Check if base_columns exist, and if not, adjust by taking what is available\n",
    "available_base_columns = [col for col in base_columns if col in base_df.columns]\n",
    "if len(available_base_columns) < 3:\n",
    "    raise Exception(\"Base DataFrame does not contain required columns (index, language, code).\")\n",
    "    \n",
    "base_df = base_df[available_base_columns].drop_duplicates(subset=[\"index\"]).copy()\n",
    "\n",
    "# Now, merge scoring information from every scoring system and every documentation model.\n",
    "# Each scoring DataFrame is keyed by \"index\". We merge them into the base_df.\n",
    "for key, df_scoring in scoring_dfs.items():\n",
    "    # Merge on \"index\" using a left join to keep all evaluation instances\n",
    "    base_df = pd.merge(base_df, df_scoring, on=\"index\", how=\"left\")\n",
    "\n",
    "# At this point, base_df has the common columns plus additional scoring columns.\n",
    "# Next, we want to reorder columns so that for each documentation model we have a block of columns:\n",
    "# 1. The documented code column (from the base)\n",
    "# 2. Followed by scoring columns from each scoring system for that model.\n",
    "final_columns = [\"index\", \"language\", \"code\"]\n",
    "for model in doc_models_order:\n",
    "    # Add the documented output column first\n",
    "    final_columns.append(model)\n",
    "    # For each scoring system, add score and reason columns.\n",
    "    for system in [\"Anthropic\", \"DeepSeek\", \"Gemini\", \"GPT4o\"]:\n",
    "        score_col = f\"{model}_{system}_score\"\n",
    "        reason_col = f\"{model}_{system}_reason\"\n",
    "        final_columns.extend([score_col, reason_col])\n",
    "\n",
    "# It is possible that some of these scoring columns are missing if not all files were present.\n",
    "final_columns = [col for col in final_columns if col in base_df.columns]\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "final_df = base_df[final_columns]\n",
    "\n",
    "# Save the final aggregated CSV\n",
    "final_csv_path = \"aggregated_evaluations.csv\"\n",
    "final_df.to_csv(final_csv_path, index=False)\n",
    "print(f\"Aggregated CSV has been saved to {final_csv_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aebfa3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bd8ace",
   "metadata": {},
   "source": [
    "# Inference Metrics Summary\n",
    "\n",
    "This table summarizes the average inference performance of different code documentation models. Each model's metrics are automatically extracted from CSV files located in the `Documentation Metrics` folder.\n",
    "\n",
    "## Metrics Explained\n",
    "\n",
    "- **Avg Prompt1 Time (s)**: Time taken to process the original input code.\n",
    "- **Avg Prompt2 Time (s)**: Time taken to generate or evaluate documentation.\n",
    "- **Avg Total Time (s)**: Combined time for both prompts.\n",
    "- **Avg Prompt1 Speed (tokens/s)**: Token processing speed for the first prompt.\n",
    "- **Avg Prompt2 Speed (tokens/s)**: Token processing speed for the second prompt.\n",
    "- **Avg Overall Speed (tokens/s)**: Average of both speeds.\n",
    "\n",
    "## File Generated\n",
    "\n",
    "ðŸ“„ `Inference_metrics.csv`  \n",
    "This file contains one row per model with all the above metrics. It can be used for comparison and visualization of model efficiency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2553a700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Inference_metrics.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Define the folder path for all metrics CSV files\n",
    "metrics_folder = \"Documentation Metrics\"\n",
    "metrics_files = glob.glob(os.path.join(metrics_folder, \"metrics_*.csv\"))\n",
    "\n",
    "# Load and compute inference metrics for each model file in the folder\n",
    "def compute_metrics_from_file(file_path):\n",
    "    model_name = os.path.basename(file_path).replace(\"metrics_\", \"\").replace(\".csv\", \"\")\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    avg_prompt1_time = df[\"prompt1_eval_duration_sec\"].mean()\n",
    "    avg_prompt2_time = df[\"prompt2_eval_duration_sec\"].mean()\n",
    "    avg_total_time = avg_prompt1_time + avg_prompt2_time\n",
    "\n",
    "    avg_prompt1_speed = df[\"prompt1_tokens_per_sec\"].mean()\n",
    "    avg_prompt2_speed = df[\"prompt2_tokens_per_sec\"].mean()\n",
    "    avg_total_speed = (avg_prompt1_speed + avg_prompt2_speed) / 2\n",
    "\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Avg Prompt1 Time (s)\": round(avg_prompt1_time, 2),\n",
    "        \"Avg Prompt2 Time (s)\": round(avg_prompt2_time, 2),\n",
    "        \"Avg Total Time (s)\": round(avg_total_time, 2),\n",
    "        \"Avg Prompt1 Speed (tokens/s)\": round(avg_prompt1_speed, 2),\n",
    "        \"Avg Prompt2 Speed (tokens/s)\": round(avg_prompt2_speed, 2),\n",
    "        \"Avg Overall Speed (tokens/s)\": round(avg_total_speed, 2),\n",
    "    }\n",
    "\n",
    "# Compute metrics for all models\n",
    "inference_metrics_all = [compute_metrics_from_file(f) for f in metrics_files]\n",
    "inference_metrics_df = pd.DataFrame(inference_metrics_all)\n",
    "\n",
    "# Save the summary CSV\n",
    "inference_output_path = \"Inference_metrics.csv\"\n",
    "inference_metrics_df.to_csv(inference_output_path, index=False)\n",
    "\n",
    "inference_output_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SUPRA",
   "language": "python",
   "name": "supra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
